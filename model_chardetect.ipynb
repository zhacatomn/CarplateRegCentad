{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras import datasets, layers, models, preprocessing, backend\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import datasets, layers, models, preprocessing, backend\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import PIL.ImageOps\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing with singapore carplate\n",
    "# generate the cropped letter images from carplates\n",
    "with open(\"cropped/data_bounding_box.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    big_count = 0\n",
    "    for path in data:\n",
    "        plate = path.split('/')[1]\n",
    "        count = 0\n",
    "        for coord in data[path]:\n",
    "            im = Image.open(path)\n",
    "            if(not os.path.isdir('cropped/extracted_char/' + plate[count])):\n",
    "                os.mkdir('cropped/extracted_char/' + plate[count])\n",
    "            cropped = im.crop((coord[0][0], coord[0][1], coord[1][0], coord[1][1]))\n",
    "            cropped.save('cropped/extracted_char/' + plate[count] + '/' + str(big_count) + '.jpg')\n",
    "            count += 1\n",
    "            big_count += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 18\n"
     ]
    }
   ],
   "source": [
    "# get avg width and height\n",
    "avg_width, avg_height = 0, 0\n",
    "with open(\"plate_data/cropped/data_bounding_box.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    counter = 0\n",
    "    for path in data:\n",
    "        for coord in data[path]:\n",
    "            avg_width += abs(coord[0][0] - coord[1][0])\n",
    "            avg_height += abs(coord[0][1] - coord[1][1])\n",
    "            counter += 1\n",
    "    avg_width = int(avg_width / counter)\n",
    "    avg_height = int(avg_height / counter)\n",
    "print(avg_height, avg_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverting colours\n",
    "for name in os.listdir('char_data'):\n",
    "        path = 'char_data/' + name\n",
    "        img = Image.open(path)\n",
    "        PIL.ImageOps.invert(img.convert('RGB')).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3050 images belonging to 34 classes.\n",
      "Found 323 images belonging to 34 classes.\n"
     ]
    }
   ],
   "source": [
    "#keras.backend.clear_session()\n",
    "datagen = preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    fill_mode = 'constant',\n",
    "#     rotation_range = 2,\n",
    "#     zoom_range = (0.8, 0.9),\n",
    "#     width_shift_range = (-0.5, 0.5),\n",
    "    validation_split = 0.1\n",
    ")\n",
    "\n",
    "traingen = datagen.flow_from_directory(\n",
    "    r'./char_data/collected',\n",
    "    target_size = (30, 20),\n",
    "    #save_to_dir = './char_data/default_aug_3x2',\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "valgen = datagen.flow_from_directory(\n",
    "    r'./char_data/collected',\n",
    "    target_size = (30, 20),\n",
    "    #save_to_dir = './char_data/default_aug_3x2',\n",
    "    subset = 'validation'\n",
    ")\n",
    "\n",
    "#testing the generator\n",
    "# i = 0\n",
    "# for batch in datagen.flow_from_directory(\n",
    "#     r'./generator',\n",
    "#     target_size = (30, 20),\n",
    "#     save_to_dir = './generator',\n",
    "# ):\n",
    "#     i += 1\n",
    "#     if i > 20:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 30, 20, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 20, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 10, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4480)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4588544   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 34)                34850     \n",
      "=================================================================\n",
      "Total params: 5,933,154\n",
      "Trainable params: 5,933,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create new model / restart model\n",
    "keras.backend.clear_session()\n",
    "input_shape = (30, 20, 3)\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "# model.add(layers.Dropout(0.20))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "# model.add(layers.Dropout(0.20))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "# model.add(layers.Dropout(0.20))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "# model.add(layers.Dropout(0.20))\n",
    "# model.add(layers.Dense(34, activation='softmax'))\n",
    "model = models.Sequential([\n",
    "layers.Conv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'),\n",
    "layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "# layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "# layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "# layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "# layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "# layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "# layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "# layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "# layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "# layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "# layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "# layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "# layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "layers.Flatten(),\n",
    "layers.Dense(1024, activation='relu'),\n",
    "# layers.Dropout(0.20),    \n",
    "layers.Dense(1024, activation='relu'),\n",
    "# layers.Dropout(0.20),\n",
    "layers.Dense(34, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"charcollectedvgg.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 20s 396ms/step - loss: 2.4713 - acc: 0.3325 - val_loss: 0.9274 - val_acc: 0.6735\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67354, saving model to charcollectedvgg.h5\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 28s 567ms/step - loss: 0.4763 - acc: 0.8654 - val_loss: 0.4763 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.67354 to 0.89003, saving model to charcollectedvgg.h5\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 26s 529ms/step - loss: 0.1857 - acc: 0.9575 - val_loss: 0.1653 - val_acc: 0.9450\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.89003 to 0.94502, saving model to charcollectedvgg.h5\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 18s 369ms/step - loss: 0.1419 - acc: 0.9575 - val_loss: 0.1389 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.94502 to 0.97595, saving model to charcollectedvgg.h5\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 29s 582ms/step - loss: 0.1094 - acc: 0.9738 - val_loss: 0.1160 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.97595 to 0.98438, saving model to charcollectedvgg.h5\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 27s 540ms/step - loss: 0.0501 - acc: 0.9869 - val_loss: 0.1190 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.98438\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 19s 382ms/step - loss: 0.0533 - acc: 0.9831 - val_loss: 0.1394 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.98438\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 29s 578ms/step - loss: 0.0888 - acc: 0.9761 - val_loss: 0.1290 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.98438\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 24s 478ms/step - loss: 0.0919 - acc: 0.9838 - val_loss: 0.0806 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.98438\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 19s 382ms/step - loss: 0.0654 - acc: 0.9787 - val_loss: 0.2518 - val_acc: 0.9588\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.98438\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 31s 623ms/step - loss: 0.0446 - acc: 0.9881 - val_loss: 0.1199 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.98438\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 20s 407ms/step - loss: 0.0408 - acc: 0.9906 - val_loss: 0.1218 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.98438 to 0.98625, saving model to charcollectedvgg.h5\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 20s 406ms/step - loss: 0.0363 - acc: 0.9894 - val_loss: 0.0526 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.98625\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 32s 640ms/step - loss: 0.0266 - acc: 0.9912 - val_loss: 0.1867 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.98625\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 0.0632 - acc: 0.9869 - val_loss: 0.0982 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.98625\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 20s 400ms/step - loss: 0.0685 - acc: 0.9800 - val_loss: 0.1606 - val_acc: 0.9625\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.98625\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 33s 663ms/step - loss: 0.0516 - acc: 0.9900 - val_loss: 0.0734 - val_acc: 0.9691\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.98625\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 20s 408ms/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.1098 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.98625\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 21s 417ms/step - loss: 0.0364 - acc: 0.9944 - val_loss: 0.1671 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.98625\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 32s 648ms/step - loss: 0.0231 - acc: 0.9956 - val_loss: 0.1176 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.98625\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 0.0296 - acc: 0.9937 - val_loss: 0.0983 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.98625 to 0.99313, saving model to charcollectedvgg.h5\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 21s 416ms/step - loss: 0.0163 - acc: 0.9975 - val_loss: 0.1058 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.99313\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 33s 660ms/step - loss: 0.0150 - acc: 0.9962 - val_loss: 0.1190 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.99313\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 17s 350ms/step - loss: 0.0546 - acc: 0.9888 - val_loss: 0.0647 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.99313\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 20s 406ms/step - loss: 0.0123 - acc: 0.9962 - val_loss: 0.0940 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.99313\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 32s 631ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.1144 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.99313\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 17s 348ms/step - loss: 0.0102 - acc: 0.9987 - val_loss: 0.0876 - val_acc: 0.9906\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.99313\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 21s 428ms/step - loss: 0.0136 - acc: 0.9969 - val_loss: 0.0970 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.99313\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 32s 633ms/step - loss: 0.0282 - acc: 0.9931 - val_loss: 0.0490 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.99313 to 0.99656, saving model to charcollectedvgg.h5\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 18s 356ms/step - loss: 0.0191 - acc: 0.9963 - val_loss: 0.1255 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.99656\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 23s 454ms/step - loss: 0.0326 - acc: 0.9931 - val_loss: 0.1415 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.99656\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 30s 596ms/step - loss: 0.0297 - acc: 0.9919 - val_loss: 0.1357 - val_acc: 0.9656\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.99656\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 17s 347ms/step - loss: 0.1210 - acc: 0.9761 - val_loss: 0.1064 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.99656\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 22s 449ms/step - loss: 0.0345 - acc: 0.9894 - val_loss: 0.1806 - val_acc: 0.9588\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.99656\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 30s 605ms/step - loss: 0.0079 - acc: 0.9975 - val_loss: 0.1512 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.99656\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 17s 350ms/step - loss: 5.8157e-04 - acc: 1.0000 - val_loss: 0.0309 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.99656\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 22s 437ms/step - loss: 1.5501e-04 - acc: 1.0000 - val_loss: 0.0926 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.99656\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 27s 533ms/step - loss: 5.0789e-05 - acc: 1.0000 - val_loss: 0.0841 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.99656\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 18s 352ms/step - loss: 1.5588e-05 - acc: 1.0000 - val_loss: 0.0352 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.99656\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 24s 477ms/step - loss: 1.5842e-05 - acc: 1.0000 - val_loss: 0.0920 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.99656\n",
      "Epoch 41/100\n",
      " 9/50 [====>.........................] - ETA: 28s - loss: 2.4414e-05 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-244-ed68ce7942f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraingen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(steps_per_epoch=50, generator=traingen, validation_data=valgen, validation_steps=10, epochs=100, callbacks=[checkpoint, early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# upload the new architecture (to vgg)\n",
    "model_json = model.to_json()\n",
    "with open(\"charcollectedvgg.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "# model.save_weights(\"def2.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get best weight\n",
    "# keras.backend.clear_session()\n",
    "json_file = open('charcollectedvgg.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "tmodel = models.model_from_json(loaded_model_json)\n",
    "tmodel.load_weights(\"charcollectedvgg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B [  1.47613100e-09   3.20274083e-08   7.45977729e-07   2.47700651e-08\n",
      "   3.71003246e-08   4.22565455e-10   2.79002439e-08   9.68104086e-09\n",
      "   2.65024234e-08   1.10002429e-09   1.45485499e-08   9.99864578e-01\n",
      "   2.62613181e-10   1.34290080e-04   1.66245306e-09   2.25510985e-10\n",
      "   1.20348467e-10   2.41800624e-09   4.22031811e-12   1.36978144e-08\n",
      "   3.20022758e-10   7.58000507e-09   1.01778119e-09   1.01001596e-07\n",
      "   1.70639503e-09   4.24311999e-08   8.65676475e-10   4.73496709e-09\n",
      "   6.68748079e-09   5.81902304e-09   3.90668331e-10   3.83828885e-10\n",
      "   6.76634582e-10   1.08849774e-08]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALYAAAD8CAYAAADaM14OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = Image.open('char_data/collected/B/1042.jpg')\n",
    "im = im.resize((20, 30))\n",
    "draw = ImageDraw.Draw(im)\n",
    "plt.figure()\n",
    "plt.imshow(np.asarray(im))\n",
    "ini = np.array([np.asarray(im)]) / 255\n",
    "out = tmodel.predict(ini)[0]\n",
    "dicta = '0123456789ABCDEFGHJKLMNPQRSTUVWXYZ'\n",
    "maxi = 0\n",
    "idx = 0\n",
    "for i in range(0, len(out)):\n",
    "    if i == 0:\n",
    "        maxi = out[i]\n",
    "    elif(out[i] > maxi):\n",
    "        maxi = out[i]\n",
    "        idx = i\n",
    "print(dicta[idx], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3373 images belonging to 34 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    fill_mode = 'constant',\n",
    "    rotation_range = 5,\n",
    "    zoom_range = (0.8, 1.2)\n",
    ")\n",
    "\n",
    "testgen = datagen.flow_from_directory(\n",
    "    r'./char_data/collected',\n",
    "    target_size = (30, 20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973584346283\n"
     ]
    }
   ],
   "source": [
    "tmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\"])\n",
    "meanacc = 0\n",
    "for i in range(10):\n",
    "    meanacc += tmodel.evaluate_generator(testgen, steps=3373/32)[1]\n",
    "meanacc /= 10\n",
    "print(meanacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 97 percent\n",
      "1: 100 percent\n",
      "2: 100 percent\n",
      "3: 99 percent\n",
      "4: 100 percent\n",
      "5: 100 percent\n",
      "6: 100 percent\n",
      "7: 100 percent\n",
      "8: 99 percent\n",
      "9: 100 percent\n",
      "A: 97 percent\n",
      "B: 100 percent\n",
      "C: 100 percent\n",
      "D: 100 percent\n",
      "E: 100 percent\n",
      "F: 100 percent\n",
      "G: 97 percent\n",
      "H: 100 percent\n",
      "J: 98 percent\n",
      "K: 100 percent\n",
      "L: 100 percent\n",
      "M: 98 percent\n",
      "N: 100 percent\n",
      "P: 99 percent\n",
      "Q: 100 percent\n",
      "R: 100 percent\n",
      "S: 99 percent\n",
      "T: 100 percent\n",
      "U: 100 percent\n",
      "V: 100 percent\n",
      "W: 100 percent\n",
      "X: 100 percent\n",
      "Y: 100 percent\n",
      "Z: 100 percent\n"
     ]
    }
   ],
   "source": [
    "dirr = 'char_data/collected'\n",
    "for cur_char in os.listdir(dirr):\n",
    "        correct = 0\n",
    "        for img in os.listdir(dirr + '/' + cur_char):\n",
    "            im = Image.open(dirr + '/' + cur_char + '/' + img)\n",
    "            im = im.resize((20, 30), Image.ANTIALIAS)\n",
    "            ini = np.array([np.asarray(im)]) / 255\n",
    "            out = tmodel.predict(ini)[0]\n",
    "            dicta = '0123456789ABCDEFGHJKLMNPQRSTUVWXYZ'\n",
    "            maxi = 0\n",
    "            idx = 0\n",
    "            for i in range(0, len(out)):\n",
    "                if i == 0:\n",
    "                    maxi = out[i]\n",
    "                elif(out[i] > maxi):\n",
    "                    maxi = out[i]\n",
    "                    idx = i\n",
    "            correct += 1 if dicta[idx] == cur_char else 0\n",
    "        total = len(os.listdir(dirr + '/' + cur_char))\n",
    "        if total != 0:\n",
    "            print('%c: %i percent' % (cur_char, round(correct / total * 100, 2)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = backend.function([model.layers[0].input, backend.learning_phase()], [model.layers[-1].output])\n",
    "def predict_with_uncertainty(f, x, n_iter=10):\n",
    "    result = np.zeros((n_iter,) + x.shape)\n",
    "    for iter in range(n_iter):\n",
    "        result[iter] = f(x, 1)\n",
    "\n",
    "    prediction = result.mean(axis=0)\n",
    "    uncertainty = result.var(axis=0)\n",
    "    return prediction, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1,1,36) into shape (1,32,32,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-16b21620ee90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mini\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_with_uncertainty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mini\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mini\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-e40a91cf3169>\u001b[0m in \u001b[0;36mpredict_with_uncertainty\u001b[1;34m(f, x, n_iter)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (1,1,36) into shape (1,32,32,3)"
     ]
    }
   ],
   "source": [
    "img = \"test/_1306_4084982.png\"\n",
    "im = Image.open(img)\n",
    "\n",
    "ini = np.array([np.asarray(im)]) / 255\n",
    "print(predict_with_uncertainty(f, ini))\n",
    "\n",
    "out = model.predict(ini)[0]\n",
    "dicta = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "maxi = 0\n",
    "idx = 0\n",
    "print(out)\n",
    "for i in range(0, len(out)):\n",
    "    if i == 0:\n",
    "        maxi = out[i]\n",
    "    elif(out[i] > maxi):\n",
    "        maxi = out[i]\n",
    "        idx = i\n",
    "print(dicta[idx])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
